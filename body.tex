\section{Introduction}

This document is meant to summarize plans for Bulk Data Transfer from the LSST United States Data Facility (USDF).
Throughout the foundation documents Bulk Transfer is used to describe the means through which the data comprising
the LSST Data Releases (DRs) will be distributed to Data Access Centers (DACs).  This document is organized as follows:
Section~\ref{sec_reqs} summarizes the current formal requirements found within the foundation documents, Section~\ref{sec_scale}
draws upon the current sizing model to set the size of the datasets that must be handled by Bulk Transfer, Section~\ref{sec_method}
describes the current methods being evaluated to manage the actual data movement, and Section~\ref{sec_users} briefly 
describes what individual users and groups can expect to be available.

\section{The Requirements\label{sec_reqs}}

The baseline requirement from the bulk data distribution is OSS-REQ-0178 which is stated in two locations.  
In the LSST Systems Requirement Document \citedsp{LSE-29} it is stated as:

\begin{quote}
2.7.1.5.2 Data Distribution (ID: LSR-REQ-0053)\\
Requirement: The LSST shall permit and facilitate the bulk distribution of its public data to 
remote sites or users wishing to consume or host it, subject to the availability of resources and the data
access policy from LSR-REQ-0052 (which states the contents of a public data release).\\
Discussion: This requirement is not intended to create an open-ended obligation to add bandwidth for data distribution. In cases where remote sites wish to host a large amount of LSST public data, it is anticipated that some cost-recovery arrangement may be needed to support the installation of additional data distribution capacity.\\
\end{quote}

Similarly, the Observatory System Specifications (\citedsp{LSE-30}) states:
\begin{quote}
3.1.7.1.2 Data Distribution (ID: OSS-REQ-0178)\\
Specification: The LSST Project shall facilitate the distribution of its data products in bulk to other sites and institutions willing to host it, in accordance with LSST Management Board approved policies for data release.\\
Discussion: This does not require the LSST Project to absorb the marginal costs of the actual distribution, e.g., the provision of increased network bandwidth between the Archive Center and the external consumer.\\
\end{quote}

And finally, in the Data Management System Requirements \citedsp{LSE-61} specifies:
\begin{quote}
3.3.2.3 Bulk Download Service (ID: DMS-REQ-0300)\\
Specification: The DMS shall provide software to enable bulk download of data products and raw data, subject to network bandwidth.\\
Discussion: This includes bulk download of older data releases.
\end{quote}

and 

\begin{quote}
4.1.16 Maintain Archive Publicly Accessible (ID: DMS-REQ-0077 \\
Specification: All releases of the DMS catalog archive shall be maintained and preserved in a publicly accessible state for the entire operational life of the LSST observatory.\\
Discussion: The scientific intent is satisfied by keeping data products from the current DRP release and the one prior available with low-latency, in a form readily queryable by the public. Earlier releases may be available from deep-store with potentially high latency, for bulk download by users.
\end{quote}

The actual data volume that will need to be moved/replicated would encompass all Alert Processing (AP) and Data Release Products (DRP) 
and the locations that would receive such bulk downloads would be the US Data Archive Center (DAC), the IN2P3 DAC (France), and the Chilean DAC 
(the latter of which is specified to hold roughly 10\% of a data release... {\bf CHECK}).  It is anticipated that further agreements may 
expand the number of DAC's but the necessary infrastructure (e.g., networking/bandwidth) to support such centers would be part of those agreements.
Bulk Transfer for any Data Release is also meant to be possible for any third party with the proviso that the cost for bandwidth and effort 
needed to accomplish such transfers would fall to such parties.


\section{Data Volume\label{sec_scale}}

The volume of data within a DR can be estimated based on the inputs to the updated sizing model \citedsp{DMTN-135}.  
For the purposes of this document we take a DR to be comprised of a set of COADD images and Parquet files (i.e. catalogs).  
In addition, the raw images and Processed Visit Images (PVI) may optionally be retained as part of a DAC's holdings.  
Of these, the COADD images remain roughly fixed in size, ~8 PB (lossless-compressed) from one release to another,
while the volume of the other products grow, as the number of observations increase linearly and the number of measurements 
should increase and N$^{3/2}$ (both due to the number observations and the increase in in depth as the survey progresses).  
The current models suggest that the raw data (lossless-compressed) will require $\sim 5\times N[yr]$~PB, that the PVI 
images (with a lossy compression) would require $\sim 8\times N[yr]$~PB, while the Parquet/catalog files would 
require $\sim 8\times N[yr]$~PB (this later case is because the updated sizing model makes a slight over-estimate 
because it scales based on the final HSC survey products).  In Table~\ref{tab_volume} these are summarized for DR1, DR2, 
DR(Y), and DR10 with two likely DAC types: Full DAC comprised of all products, and Minimal DAC comprised of only COADD 
and Catalog products.  Note DR(Y) is given to provide formulaic values for intermediate releases in each of the Y release years.

\begin{table}[!ht]
\caption{Summary of Expected Volume of LSST Release Products {\bf THESE ITEMS NEED TO BE CHECKED }}
\label{tab_volume}
\footnotesize
\centering
\begin{tabular}[]{|l|cccc|}
\hline
\hline
            &   DR1  &  DR2   &  DR(Y)  &  DR10 \\
 product    &  [PB]  &  [PB]  &  [PB]   &  [PB] \\
\hline
 raw        &    5   &   10   &  5$\times$Y    &   50   \\
 COADD      &    8   &    8   &    8           &    8   \\
 PVI        &    8   &   16   &  8$\times$Y    &   80   \\
 catalog    &    8   &   16   &  8$\times$Y    &   80   \\
\hline
 Minimal    &   16   &   24   &  8 + (8$\times$Y) &   88   \\
 Full       &   29   &   50   &  8 + (21$\times$Y) &  218  \\
\hline
\end{tabular}
\end{table}

% I am attempting to obtain a data volume expected for Bulk Transfer.   
% There I am assuming that the heaviest use would be to distribute a DR to a DAC.
% To do this I am using the sizing model (Google sheet)  and taking the OPS Storage sheet and using the Dataset Sizing.
% There for any DR I assume the amount of data needed to move prior to a DR release is the combination of the LSSTCam:
% Output COADD Images,
% Output Images, and
% Output Parquet
% Anything else? e.g. I am assuming qserv would be derived from Output Parquet?  Raw could be transferred slowly as obtained…
% And in fact the Output Images (I assumed these were PVIs) could be a choice that some DACs might not want.. (edited) 

% Kian-Tat Lim  3:10 PM
% OK, if you're using row 38 for Output Images (PVIs), that's lossless-compressed, so perhaps conservative. (edited) 
% You may also want to add row 44 "Other/Misc" which is my "contingency reserve".
% But that table does have only the current release, unlike "on the floor", which sums multiple releases, so you're in good shape there.
% (Lossy compression is supposed to get us a factor of 4 beyond lossless.)
% Yes, Qserv would be independently loaded from Output Parquet.
% The one other variable is the timeframe for transmission of a DR.  
% The two ends of the spectrum are "trickle it out as files are created, with the understanding 
% that they might be modified later" and "dump the whole thing in minimal time just before (or at or after) release".
% LSE-81 gives 30 days to transmit catalogs and ~60 to transfer templates and coadds
% But that may be unrealistic nowadays.

% Time-frame for a DRP would be O(6 months)?  for the “trickle out as created?
% And by unrealistic you mean that the 30/60 days is too long?

% LSE-81 has up to ~40 weeks for a full DRP compute.
% I'm worried on two axes: DRP may take all the time we can give it, and extensive QA may be needed before transmission to partner non-project DACs.

The worst case scenario is that all data would need to be moved from a central site to a DAC.  Assuming further that this
needs to be accomplished in a 1-month time frame, the necssary bandwidth (assuming no downtime) needed to support transfer 
is given by: 
$$ R[Gb/s] = S \times V[PB] \times 8 [b/B] \times 1024^2 / 30 \times 24 \times 3600 = 3.2 \times S \times V[PB] / T[months] $$.
Where $R$ is the necessary bandwidth to support data transfer to a data center, $S$ is the number of sites, $V$ is the volume of data, 
and $T$ is the number of months the transfer would occur over.  Thus, the bandwidth necessary
to move all of DR1 to single site over the course of one month would be 110 Gb/s, rising to 880 Gb/s per site for DR10.

Clearly, the above is wasteful.  A more reasonable scenario would assume that DACs and other data centers must be identified
well before a release (or in fact before release processing begins).  Still ignoring that many DACs might also participate 
in the data reduction and that the data comes from a central source (the USDF), it should be expected that the raw data can 
be transfered as it is acquired, and that the COADD and PVI images can like-wise be transfered while a DRP campaign is underway.
The situation for the final catalogs is less clear.  In the worst case, that where the catalog data must be held until final assembly,
calibration, normalization, and vetting have occurred, the dissemination of final catalog products would still occur in a burst at the end of the
DRP campaign.  
Therefore, the data transfer of the raw data would be spread over the course of a year, the PVI and COADD 
images would be transfered over the course of $\sim$6 months, and the final catalogs transfered in the month prior to the release.
This results in 3 different rates summarized in Table~\ref{tab_rate} where ``ambient'' refers to periods when only raw data
is being acquired, ``production'' refers to periods where DRP is ongoing (raw+COADD+PVI), and ``release'' refers to periods where 
a release is being finalized (raw+catalog).


\begin{table}[!ht]
\caption{Bandwith Needs per Data Access Site} 
\label{tab_rate}
\footnotesize
\centering
\begin{tabular}[]{|l|rrcr|}
\hline
\hline
            &   DR1   &  DR2   &  DR(Y)  &  DR10 \\
 product    &  [Gb/s] & [Gb/s] &  [Gb/s] & [Gb/s] \\
\hline
\multicolumn{5}{c}{minimal DAC} \\
\hline
 ambient    &  1.35  &  1.35  &  1.35      &  1.35  \\
 production &  5.66  &  5.66  &  5.66      &  5.66  \\
 release    & 27.24  & 53.13  &  1.35 + (25.89$\times$Y) & 260.26 \\
\hline
\multicolumn{5}{c}{Full DAC} \\
\hline
 ambient    &  1.35  &  1.35  &  1.35      &  1.35  \\
 production & 10.00  & 14.33  &  5.67 + (4.34$\times$Y) &  49.07 \\
 release    & 27.24  & 53.13  &  1.35 + (25.89$\times$Y) & 260.26 \\
\hline
\end{tabular}
\end{table}

The only major downside to much of the data distribution occuring throughout the DRP campaign is that it will likely entail that
any re-processing deemed necessary to fix problems during DRP would also require more data transfer of those amended results.
Furthermore, it would require that mechanisms exists to prune the holdings within the DAC as new results replace those from the 
prior problematic processing attempts.  

One futher econimization is possible, that would entail that catalog results also are part of data distribution which occurs within
the ``production'' phase.  The caveat being that any amendations that would have occurred during the final assembly would need to 
occur at each of the data facilities.  Such decentralized processes are not specifically envisaged in the current data management 
and DAC system (where a US and Chilean DAC are the only mandated centers).  While such an addition is not neccesary in the first
years of operations it should be considered that the development of such may be necessary in the later years of the projet.


\section{Data Curation/Replication Strategy\label{sec_method}}

Within the LSST data management project, files are tracked and maintained by the Data Backbone (DBB).  Within the current project
the DBB endpoints include the USDF (USDAC), the Chilean DAC, and CC-IN2P3 (in France).  Currently the USDF and CC-IN2P3 plan to 
maintain a complete set of files and hence act as a backup to all sites.  The current method for replication/tracking uses Rucio.
Any further computation sites that are identified within the project would also act as DBB endpoints but are not necessarily required
to maintain full data sets.

Rucio was developed to enable data management with the CERN/ATLAS project and successfully manages the data movement 
and replication needs within the ATLAS Tiered centers.  This is accomplished by connecting a database with a datastore.
Traditional files are assigned Data Identifiers and the further inter-relationships are defined as datasets (groups of 
files), containers (groups of datasets or other containers).   Data replication and management at different sites are then
maintained through rules that use the database to define the needed data movement and curation among sites.  The actual 
transfer mechanism used within Rucio is not fixed and there are current efforts to integrate modern parallel methods 
(e.g., Big Data Express).  

Within the LSST project the proposed method to use Rucio is to link data access within the context of the LSST Gen3 Butler 
through a Rucio Data Identifier (DID).  Thus, data management/replication between sites would be implemented through Rucio.  
While Rucio is capable of storing limited metadata and provenance, the current risk is that both a Rucio system and LSST Butler would
need to be maintained alongside one another, thus imposing two sources of risk/effort.  First, where information/communication
between the two systems are necesssary, software services would be necessary, and second, the natural evolution of both
the LSST software and Rucio software will require long-term effort to maintain the interoperability of the two.

The most economical way to provide bulk transfer to sites not engaged in LSST computation is to extend the utilization of Rucio to 
replicate subsets of the processing products to such sites as production is on-going.  Therefore it should be required that:
\begin{enumerate}
\item Access to proprietary data is only allowed to those with data rights.
\item Prior to release, data being staged to a site is not made available to users,
\item The cost of bandwidth needed to replicate data to non-project sites is defrayed.
\item Services to manage replication/tracking are deployed at each site.
\item Older data releases (i.e. those beyond their proprietary period) would be available through some other service.
\end{enumerate}

\subsection{Current Questions/Concerns}


\section{Support for Non-DAC Data Download\label{sec_users}}

This section is a place holder to express the options/scales for data download for users/projects.
A range of ideas that should be considered is the availability of data to compute resources outside of DACs.
For instance: 1) to DESC and DOE compute resources, 2) to users groups with access to EXSEDE and OSG 
resources, 3) to users with other compute resources at their discretion.






