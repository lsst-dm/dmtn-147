\section{Introduction}

This document is meant to summarize plans for Bulk Data Transfer from the LSST United States Data 
Facility (USDF).  Throughout the foundation documents Bulk Transfer is used to describe the means 
through which the data comprising the LSST Data Releases (DRs) will be distributed to Data Access 
Centers (DACs).  This document is organized as follows: Section~\ref{sec_reqs} summarizes the 
current formal requirements found within the foundation documents, Section~\ref{sec_scale}
draws upon the current sizing model to set the size of the data sets that must be handled by 
Bulk Transfer, and Section~\ref{sec_method} describes the current methods being evaluated to manage 
the actual data movement.

\section{The Requirements\label{sec_reqs}}

The baseline requirement from the bulk data distribution is OSS-REQ-0178 which is stated in two locations.  
In the LSST Systems Requirement Document \citedsp{LSE-29} it is stated as:

\begin{quote}
2.7.1.5.2 Data Distribution (ID: LSR-REQ-0053)\\
Requirement: The LSST shall permit and facilitate the bulk distribution of its public data to 
remote sites or users wishing to consume or host it, subject to the availability of resources 
and the data access policy from LSR-REQ-0052 (which states the contents of a public data release).\\
Discussion: This requirement is not intended to create an open-ended obligation to add bandwidth 
for data distribution. In cases where remote sites wish to host a large amount of LSST public data, 
it is anticipated that some cost-recovery arrangement may be needed to support the installation of 
additional data distribution capacity.\\
\end{quote}

Similarly, the Observatory System Specifications (\citedsp{LSE-30}) states:
\begin{quote}
3.1.7.1.2 Data Distribution (ID: OSS-REQ-0178)\\
Specification: The LSST Project shall facilitate the distribution of its data products in bulk to 
other sites and institutions willing to host it, in accordance with LSST Management Board approved
policies for data release.\\
Discussion: This does not require the LSST Project to absorb the marginal costs of the actual 
distribution, e.g., the provision of increased network bandwidth between the Archive Center and 
the external consumer.\\
\end{quote}

And finally, in the Data Management System Requirements \citedsp{LSE-61} specifies:
\begin{quote}
3.3.2.3 Bulk Download Service (ID: DMS-REQ-0300)\\
Specification: The DMS shall provide software to enable bulk download of data products 
and raw data, subject to network bandwidth.\\
Discussion: This includes bulk download of older data releases.
\end{quote}

and 

\begin{quote}
4.1.16 Maintain Archive Publicly Accessible (ID: DMS-REQ-0077 \\
Specification: All releases of the DMS catalog archive shall be maintained and preserved 
in a publicly accessible state for the entire operational life of the LSST observatory.\\
Discussion: The scientific intent is satisfied by keeping data products from the current 
DRP release and the one prior available with low-latency, in a form readily query-able by 
the public. Earlier releases may be available from deep-store with potentially high latency, 
for bulk download by users.
\end{quote}

The actual data volume that will need to be moved/replicated would encompass the raw data,
Alert Production (AP) results and the Data Release Products (DRP).  Currently, the locations that
would receive bulk downloads are the US Data Archive Center (DAC), the IN2P3 DAC (France), and 
the Chilean DAC.  It is anticipated that further agreements may expand the number of DAC's but 
the necessary infrastructure (e.g., networking/bandwidth) to support such centers would be part 
of those agreements.  Bulk Transfer for any publicly-accessible Data Release (i.e. after the 
proprietary period) is to be possible 
for any third party with the proviso that the cost for bandwidth and effort needed to accomplish 
such transfers would fall to such parties and with the understanding that the source of such
archival releases would likely be from long-term storage (e.g. tape archive or cloud resource) that might
further limit the rate the data could be provided.


\section{Data Volume\label{sec_scale}}

Throughout this section, the estimates used are based upon the updated sizing model \citedsp{DMTN-135}.

The volume of data generated in Alert Production that would be made available to the DACs
consists of the Alert Production Data Base.  The APDB size is estimated as 24 TB, representing
a 12 month store of Alerts.  Therefore, the simple estimate is that this would change by $\sim$70~GB
per night.  As this is much smaller than the raw and DRP volumes it can be ignored except 
for questions that surround how the difference images are made available. {\bf (Need to understand
whether more than APDB is promised; for example difference images, cutouts? and whether the DRs 
contain separate, after-the-fact, reprocessed difference imaging analysis/products).}

For the purposes of this document we take the DR to be comprised of a set of COADD images 
and Parquet files (i.e. catalogs).  In addition, the raw images 
and Processed Visit Images (PVI) may optionally be retained as part of a DAC's holdings.  
Of these, the COADD images remain roughly fixed in size, ~8 PB (lossless-compressed) from 
one release to another, while the volume of the other products grow, as the number of 
observations increase linearly and the number of measurements should increase and N$^{3/2}$ 
(both due to the number observations and the increase in in depth as the survey progresses).  
The current models suggest that the raw data (lossless-compressed) will require 
$\sim 5\times N[yr]$~PB, that the PVI images (with a lossy compression) would require 
$\sim 8\times N[yr]$~PB, while the Parquet/catalog files would require $\sim 8\times N[yr]$~PB 
(this later case is because the updated sizing model makes a slight over-estimate because it 
scales based on the final HSC survey products).  In Table~\ref{tab_volume} these are summarized 
for DR1, DR2, DR(Y), and DR10 with two likely DAC types: Full DAC comprised of all products, 
and Minimal DAC comprised of only COADD and Catalog products.  Note DR(Y) is given to provide 
formulaic values for intermediate releases in each of the Y release years.

\begin{table}[!ht]
\caption{Summary of Expected Volume of LSST Release Products {\bf THESE ITEMS NEED TO BE CHECKED }}
\label{tab_volume}
\footnotesize
\centering
\begin{tabular}[]{|l|cccc|}
\hline
\hline
            &   DR1  &  DR2   &  DR(Y)  &  DR10 \\
 product    &  [PB]  &  [PB]  &  [PB]   &  [PB] \\
\hline
 raw        &    5   &   10   &  5$\times$Y    &   50   \\
 COADD      &    8   &    8   &    8           &    8   \\
 PVI        &    8   &   16   &  8$\times$Y    &   80   \\
 catalog    &    8   &   16   &  8$\times$Y    &   80   \\
\hline
 Minimal    &   16   &   24   &  8 + (8$\times$Y) &   88   \\
 Full       &   29   &   50   &  8 + (21$\times$Y) &  218  \\
\hline
\end{tabular}
\end{table}


The worst case scenario is that all data would need to be moved from a central site to a DAC.  
Assuming further that this needs to be accomplished in a 1-month time frame, the necessary bandwidth 
(assuming no downtime) needed to support transfer is given by: 
$$ R[Gb/s] = S \times V[PB] \times 8 [b/B] \times 1024^2 / 30 \times 24 \times 3600 = 3.2 \times S \times V[PB] / T[months] $$.
Where $R$ is the necessary bandwidth to support data transfer to a data center, $S$ is the number 
of sites, $V$ is the volume of data, and $T$ is the number of months the transfer would occur over.  
Thus, the bandwidth necessary to move all of DR1 to single site over the course of one month would
be 110 Gb/s, rising to 880 Gb/s per site for DR10.

Clearly, the above is wasteful.  A more reasonable scenario would assume that DACs and other 
data centers must be identified well before a release (or in fact before release processing begins).
Still ignoring that many DACs might also participate in the data reduction and that the data comes 
from a central source (the USDF), it should be expected that the raw data can be transferred as
it is acquired, and that the COADD and PVI images can like-wise be transferred while a DRP 
campaign is underway.  The situation for the final catalogs is less clear.  In the worst case, 
that where the catalog data must be held until final assembly, calibration, normalization, and 
vetting have occurred, the dissemination of final catalog products would still occur in a burst
at the end of the DRP campaign.  Therefore, the data transfer of the raw data would be spread 
over the course of a year, the PVI and COADD images would be transferred over the course of 
$\sim$6 months, and the final catalogs transferred in the month prior to the release.  This results 
in 3 different rates summarized in Table~\ref{tab_rate} where ``ambient'' refers to periods when 
only raw data is being acquired, ``production'' refers to periods where DRP is ongoing 
(raw+COADD+PVI), and ``release'' refers to periods where a release is being finalized 
(raw+catalog).


\begin{table}[!ht]
\caption{Bandwidth Needs per Data Access Site} 
\label{tab_rate}
\footnotesize
\centering
\begin{tabular}[]{|l|rrcr|}
\hline
\hline
            &   DR1   &  DR2   &  DR(Y)  &  DR10 \\
 product    &  [Gb/s] & [Gb/s] &  [Gb/s] & [Gb/s] \\
\hline
\multicolumn{5}{c}{minimal DAC} \\
\hline
 ambient    &  1.35  &  1.35  &  1.35      &  1.35  \\
 production &  5.66  &  5.66  &  5.66      &  5.66  \\
 release    & 27.24  & 53.13  &  1.35 + (25.89$\times$Y) & 260.26 \\
\hline
\multicolumn{5}{c}{Full DAC} \\
\hline
 ambient    &  1.35  &  1.35  &  1.35      &  1.35  \\
 production & 10.00  & 14.33  &  5.67 + (4.34$\times$Y) &  49.07 \\
 release    & 27.24  & 53.13  &  1.35 + (25.89$\times$Y) & 260.26 \\
\hline
\end{tabular}
\end{table}

The only major downside to much of the data distribution occurring throughout the DRP campaign is 
that it will likely entail that any re-processing deemed necessary to fix problems during DRP 
would also require more data transfer of those amended results.  Furthermore, it would require 
that mechanisms exists to prune the holdings within the DAC as new results replace those from the 
prior problematic processing attempts.  

One further econimization is possible, that would entail that catalog results also are part 
of data distribution which occurs within the ``production'' phase.  The caveat being that any 
emendations that would have occurred during the final assembly would need to occur at each of 
the data facilities.  Such decentralized processes are not specifically envisaged in the 
current data management and DAC system (where a US and Chilean DAC are the only mandated centers).
While such an addition is not necessary in the first years of operations it should be considered 
that the development of such may be necessary in the later years of the project.


\section{Data Curation/Replication Strategy\label{sec_method}}

Within the LSST data management project, files are tracked and maintained by the Data Backbone (DBB).
Within the current project the DBB endpoints include the USDF (USDAC), the Chilean DAC, and 
CC-IN2P3 (in France).  Currently the USDF and CC-IN2P3 plan to maintain a complete set of files 
and hence act as a backup to all sites.  The current method for replication/tracking uses Rucio.
Any further computation sites that are identified within the project would also act as DBB endpoints
but are not necessarily required to maintain full data sets. 

Rucio was developed to enable data management with the CERN/ATLAS project and successfully 
manages the data movement and replication needs within the ATLAS Tiered centers.  This is 
accomplished by connecting a database with a data store.  Traditional files are assigned 
Data Identifiers and the further inter-relationships are defined as data sets (groups of 
files), containers (groups of data sets or other containers).   Data replication and management 
at different sites are then maintained through rules that use the database to define the needed
data movement and curation among sites.  The actual transfer mechanism used within Rucio 
is not fixed and there are current efforts to integrate modern parallel methods 
(e.g., \href{https://bigdataexpress.fnal.gov}{Big Data Express}).  

Within the LSST project the proposed method to use Rucio is to link data access within the context 
of the LSST Gen3 Butler through a Rucio Data Identifier (DID).  Thus, data management/replication 
between sites would be implemented through Rucio.  While Rucio is capable of storing limited metadata 
and provenance, the current risk is that both a Rucio system and LSST Butler would need to be 
maintained alongside one another, thus imposing two software services that need to be synchronized.  
In other words, for the data transferred to a remote site to be more than a large file cache, the 
appropriate Butler entries would also have to be present, as well as a utility to re-establish 
any linkages among DB/Butler, Rucio, and the file store.  Moreover, sites engaged in both DRP and 
data serving would require separate 
means (e.g. Butler repositories and possibly file-stores) for maintaining files/metadata that are 
part of production and those that are part of a release.  

The most economical way to provide bulk transfer to sites not engaged in LSST computation is to 
extend the utilization of Rucio to replicate subsets of the processing products to such sites as 
production is on-going.  Therefore it should be required that:
\begin{enumerate}
\item Access to proprietary data is only allowed to those with data rights.
\item Prior to release, data being staged to a site is not made available to users,
\item The cost of bandwidth needed to replicate data to non-project sites is defrayed.
\item Services to manage replication/tracking are deployed at each site.
\item Older data releases (i.e. those beyond their proprietary period) would be available through some other service.
\end{enumerate}

\subsection{Current Questions/Concerns}
While the replication of files is relatively straight-forward, it is as yet undetermined how to deal
with questions about the deletion/replacement of re-processed results.  Also, the Butler infrastructure 
requires that not only data be replicated but the subset of the Butler DB be replicated.  Currently, each 
site maintaining a DR would also require someone to either provide their own back ends (to transform the data 
to their own serving infrastructure) or duplicate project infrastructure (e.g. qserv instances) in order
to serve data.








